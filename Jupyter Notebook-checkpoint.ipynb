{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandWritten Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(\"hand.csv\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trianing dataset\n",
    "xtrain =data[0:40000,1:]\n",
    "train_lebel=data[0:40000,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "xtest=data[30000:,1:]\n",
    "actual_lebel=data[30000:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML & DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Confusion Matrix for RF: \n",
      "[[1192    0    0    0    0    0    2    0    1    0]\n",
      " [   0 1349    0    0    0    1    1    0    1    0]\n",
      " [   3    0 1148    2    0    0    0    2    2    0]\n",
      " [   0    0    4 1242    1    2    3    2    2    2]\n",
      " [   0    1    1    0 1134    0    1    0    1    2]\n",
      " [   1    0    0    2    0 1067    1    0    4    1]\n",
      " [   1    0    0    0    1    0 1165    0    0    0]\n",
      " [   0    1    2    1    0    0    0 1262    0    2]\n",
      " [   0    1    1    1    1    1    2    1 1166    0]\n",
      " [   0    0    3    3    3    0    0    1    1 1202]]\n",
      "*Classification Report for RF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1195\n",
      "           1       1.00      1.00      1.00      1352\n",
      "           2       0.99      0.99      0.99      1157\n",
      "           3       0.99      0.99      0.99      1258\n",
      "           4       0.99      0.99      0.99      1140\n",
      "           5       1.00      0.99      0.99      1076\n",
      "           6       0.99      1.00      0.99      1167\n",
      "           7       1.00      1.00      1.00      1268\n",
      "           8       0.99      0.99      0.99      1174\n",
      "           9       0.99      0.99      0.99      1213\n",
      "\n",
      "    accuracy                           0.99     12000\n",
      "   macro avg       0.99      0.99      0.99     12000\n",
      "weighted avg       0.99      0.99      0.99     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(xtrain,train_lebel)\n",
    "predictions = RF.predict(xtest)\n",
    "print(\"*Confusion Matrix for RF: \")\n",
    "print(confusion_matrix(actual_lebel, predictions))\n",
    "print(\"*Classification Report for RF: \")\n",
    "print(classification_report(actual_lebel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001607717041800643\n",
      "99.39166666666667\n",
      "0.999195494770716\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(actual_lebel, predictions)\n",
    "TP = confusion[3, 3]\n",
    "TN = confusion[0, 3]\n",
    "FP = confusion[0, 8]\n",
    "FN = confusion[8, 3]\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)\n",
    "val1 = accuracy_score(actual_lebel, predictions)*100\n",
    "print(val1)\n",
    "from sklearn import metrics\n",
    "RF_sensitivity = (TP / float(FN + TP))\n",
    "\n",
    "print(RF_sensitivity)\n",
    "RF_specificity = (TN / (TN + FP))\n",
    "\n",
    "print(RF_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Confusion Matrix for DT: \n",
      "[[1175    1    1    1    1    6    3    1    5    1]\n",
      " [   0 1346    0    1    1    0    1    1    2    0]\n",
      " [   1    1 1118    8    4    1    6    6   11    1]\n",
      " [   3    3    7 1222    2    6    0    3    7    5]\n",
      " [   2    3    6    1 1101    3    5    4    2   13]\n",
      " [   3    3    4   13    3 1038    3    1    7    1]\n",
      " [   2    1    0    2    3    6 1146    2    4    1]\n",
      " [   1    1    8    1    1    0    1 1250    2    3]\n",
      " [   3    4    6    6    5    6    5    2 1132    5]\n",
      " [   1    0    3    4   14    3    0    9    3 1176]]\n",
      "*Classification Report for DT: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1195\n",
      "           1       0.99      1.00      0.99      1352\n",
      "           2       0.97      0.97      0.97      1157\n",
      "           3       0.97      0.97      0.97      1258\n",
      "           4       0.97      0.97      0.97      1140\n",
      "           5       0.97      0.96      0.97      1076\n",
      "           6       0.98      0.98      0.98      1167\n",
      "           7       0.98      0.99      0.98      1268\n",
      "           8       0.96      0.96      0.96      1174\n",
      "           9       0.98      0.97      0.97      1213\n",
      "\n",
      "    accuracy                           0.98     12000\n",
      "   macro avg       0.98      0.97      0.97     12000\n",
      "weighted avg       0.98      0.98      0.98     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT.fit(xtrain,train_lebel)\n",
    "predictions = DT.predict(xtest)\n",
    "print(\"*Confusion Matrix for DT: \")\n",
    "print(confusion_matrix(actual_lebel, predictions))\n",
    "print(\"*Classification Report for DT: \")\n",
    "print(classification_report(actual_lebel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008914100486223663\n",
      "97.53333333333333\n",
      "0.995114006514658\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(actual_lebel, predictions)\n",
    "TP = confusion[3, 3]\n",
    "TN = confusion[0, 3]\n",
    "FP = confusion[0, 8]\n",
    "FN = confusion[8, 3]\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)\n",
    "val2 = accuracy_score(actual_lebel, predictions)*100\n",
    "print(val2)\n",
    "from sklearn import metrics\n",
    "DT_sensitivity = (TP / float(FN + TP))\n",
    "\n",
    "print(DT_sensitivity)\n",
    "DT_specificity = (TN / (TN + FP))\n",
    "\n",
    "print(DT_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier - ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Confusion Matrix for MLP: \n",
      "[[1189    0    0    0    1    1    1    0    2    1]\n",
      " [   0 1337    2    3    0    1    1    1    7    0]\n",
      " [   2    1 1136    6    1    0    1    6    2    2]\n",
      " [   1    1    0 1246    0    3    0    3    4    0]\n",
      " [   0    3    0    0 1125    0    1    0    3    8]\n",
      " [   3    2    0    9    0 1057    0    0    3    2]\n",
      " [   5    0    2    0    1    0 1158    0    1    0]\n",
      " [   0    1    4    0    2    1    0 1255    0    5]\n",
      " [   4    7    5   22    2   10    3    2 1117    2]\n",
      " [   3    0    3    6   19    4    0   13    2 1163]]\n",
      "*Classification Report for MLP: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1195\n",
      "           1       0.99      0.99      0.99      1352\n",
      "           2       0.99      0.98      0.98      1157\n",
      "           3       0.96      0.99      0.98      1258\n",
      "           4       0.98      0.99      0.98      1140\n",
      "           5       0.98      0.98      0.98      1076\n",
      "           6       0.99      0.99      0.99      1167\n",
      "           7       0.98      0.99      0.99      1268\n",
      "           8       0.98      0.95      0.97      1174\n",
      "           9       0.98      0.96      0.97      1213\n",
      "\n",
      "    accuracy                           0.98     12000\n",
      "   macro avg       0.98      0.98      0.98     12000\n",
      "weighted avg       0.98      0.98      0.98     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier()\n",
    "MLP.fit(xtrain,train_lebel)\n",
    "predictions = MLP.predict(xtest)\n",
    "print(\"*Confusion Matrix for MLP: \")\n",
    "print(confusion_matrix(actual_lebel, predictions))\n",
    "print(\"*Classification Report for MLP: \")\n",
    "print(classification_report(actual_lebel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01889763779527559\n",
      "98.19166666666666\n",
      "0.9826498422712934\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(actual_lebel, predictions)\n",
    "TP = confusion[3, 3]\n",
    "TN = confusion[0, 3]\n",
    "FP = confusion[0, 8]\n",
    "FN = confusion[8, 3]\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)\n",
    "val3 = accuracy_score(actual_lebel, predictions)*100\n",
    "print(val3)\n",
    "from sklearn import metrics\n",
    "MLP_sensitivity = (TP / float(FN + TP))\n",
    "\n",
    "print(MLP_sensitivity)\n",
    "MLP_specificity = (TN / (TN + FP))\n",
    "\n",
    "print(MLP_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(xtrain,train_lebel)\n",
    "predictions = KNN.predict(xtest)\n",
    "print(\"*Confusion Matrix for KNN: \")\n",
    "print(confusion_matrix(actual_lebel, predictions))\n",
    "print(\"*Classification Report for KNN: \")\n",
    "print(classification_report(actual_lebel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(actual_lebel, predictions)\n",
    "TP = confusion[3, 3]\n",
    "TN = confusion[0, 3]\n",
    "FP = confusion[0, 8]\n",
    "FN = confusion[8, 3]\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)\n",
    "val4 = accuracy_score(actual_lebel, predictions)*100\n",
    "print(val4)\n",
    "from sklearn import metrics\n",
    "KNN_sensitivity = (TP / float(FN + TP))\n",
    "\n",
    "print(KNN_sensitivity)\n",
    "KNN_specificity = (TN / (TN + FP))\n",
    "\n",
    "print(KNN_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bring some raw data.\n",
    "frequencies = [val1,val2,val3,val4]\n",
    "\n",
    "# In my original code I create a series and run on that,\n",
    "# so for consistency I create a series from the list.\n",
    "freq_series = pd.Series.from_array(frequencies)\n",
    "\n",
    "x_labels = ['RF','DT','MLP-ANN','KNN']\n",
    "\n",
    "# Plot the figure.\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = freq_series.plot(kind='bar')\n",
    "ax.set_title('Evaluation of ML & DL')\n",
    "ax.set_xlabel('Classifier!')\n",
    "ax.set_ylabel('Accuracy Range')\n",
    "ax.set_xticklabels(x_labels)\n",
    "\n",
    "\n",
    "def add_value_labels(ax, spacing=5):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.1f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "\n",
    "# Call the function above. All the magic happens there.\n",
    "add_value_labels(ax)\n",
    "plt.show()\n",
    "#plt.savefig(\"image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy - Sensitivity - Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 4\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.2      # the width of the bars\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "yvals = [val1,val2,val3,val4]\n",
    "\n",
    "rects1 = ax.bar(ind, yvals, width, color='r')\n",
    "zvals = [RF_sensitivity,DT_sensitivity,MLP_sensitivity,KNN_sensitivity]\n",
    "rects2 = ax.bar(ind+width, zvals, width, color='g')\n",
    "kvals = [RF_specificity,DT_specificity,MLP_specificity,KNN_specificity]\n",
    "rects3 = ax.bar(ind+width*2, kvals, width, color='b')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(ind+width)\n",
    "ax.set_xticklabels( ('RF','DT','MLP-ANN','KNN') )\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0]), ('Accuracy', 'Sensitvity', 'Specificity') )\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2., 1.05*h, '%d'%int(h),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from werkzeug.wrappers import Request, Response\n",
    "import time\n",
    "import os\n",
    "from flask import Flask, render_template, request\n",
    "from ocr_core import ocr_core # import our OCR function\n",
    "from model import hdr_predition\n",
    "from model import hdr_accuracy\n",
    "from model import hdr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a folder to store and later serve the images\n",
    "UPLOAD_FOLDER = '/static/uploads/'\n",
    "\n",
    "# allow files of a specific type\n",
    "ALLOWED_EXTENSIONS = set(['png', 'jpg', 'jpeg'])\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# function to check the file extension\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route and function to handle the home page\n",
    "@app.route('/')\n",
    "def home_page():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route and function to handle the upload page\n",
    "@app.route('/upload', methods=['GET', 'POST'])\n",
    "def upload_page():\n",
    "    start = time.time()\n",
    "    if request.method == 'POST':\n",
    "        # check if there is a file in the request\n",
    "        if 'file' not in request.files:\n",
    "            return render_template('upload.html', msg='No file selected')\n",
    "        file = request.files['file']\n",
    "        # if no file is selected\n",
    "        if file.filename == '':\n",
    "            return render_template('upload.html', msg='No file selected')\n",
    "\n",
    "        if file and allowed_file(file.filename):\n",
    "\n",
    "            # call the OCR function on it\n",
    "            extracted_text = ocr_core(file)\n",
    "            end = time.time()\n",
    "            predition_time =round(end-start,3)\n",
    "            # extract the text and display it\n",
    "            return render_template('upload.html',\n",
    "                                   msg='Successfully processed',\n",
    "                                   predition_time=predition_time,\n",
    "                                   extracted_text=extracted_text,\n",
    "                                   img_src=UPLOAD_FOLDER + file.filename)\n",
    "    elif request.method == 'GET':\n",
    "        return render_template('upload.html')\n",
    "\n",
    "# route and function to handle the digit page\n",
    "@app.route('/digit', methods=['GET', 'POST'])\n",
    "def digit_page():\n",
    "    start = time.time()\n",
    "    if request.method == 'POST':\n",
    "        index = request.form['index']\n",
    "        predition = hdr_predition(index)\n",
    "        accuracy = hdr_accuracy()\n",
    "        img_src=hdr_img(index)\n",
    "        end = time.time()\n",
    "        predition_time =round(end-start,3)\n",
    "        # extract the text and display it\n",
    "        return render_template('digit.html',\n",
    "                                predition=predition,\n",
    "                                accuracy=accuracy,\n",
    "                                predition_time=predition_time,\n",
    "                                img_src=img_src)\n",
    "    elif request.method == 'GET':\n",
    "        return render_template('digit.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\tapp.debug = True\n",
    "\t#app.run()\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
